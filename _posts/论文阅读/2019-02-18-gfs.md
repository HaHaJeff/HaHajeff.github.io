---
?layout: post
title: gfs论文阅读
category: 论文阅读
tags: [分布式系统]
keywords: 分布式系统, google
---
---

# google file system

## 背景

- 系统存储一定数量的**大文件**
- 系统的工作负载主要由两种读操作组成：**大规模的流式读取和小规模的随机读取**
  - 大规模流式读取通常一次读取数百KB的数据，更常见的是一次读取1MB甚至更多的数据
  - 小规模的随机读取通常是在文件某个随机的位置读取几个KB数据

## 架构

![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/gfs/GFS_Architecture.png)

- 一个GFS集群包含一个单独的Master节点、多台chunk服务器。
- GFS存储的文件都被分割成固定大小的Chunk。在Chunk创建的时候，Master服务器会给每个Chunk分配一个不变的、全球唯一的64位Chunk标识。
- Chunk服务器把Chunk以文件的形式保存在本地硬盘上，并且根据指定的chunk标识和字节范围来读写块数据。
- 出于**可靠性**的考虑，每个chunk都会复制到多个chunk服务器上，默认使用3副本进行保存。

### Master节点

Master节点管理文件系统的所有**元数据**信息，这些元数据包括：

- 名字空间（指文件系统的目录结构等信息）
- 访问控制信息
- 文件和chunk的映射信息
- 当前chunk的位置信息

与此同时，Master还管理着系统范围内的活动，比如：

- chunk的lease分配
- 过时chunk的回收
- chunk在chunk服务器之间的迁移

Master节点使用**心跳信息**周期地和每个chunk服务器进行通讯，发送指令到各个chunk服务器并接收chunk服务器的状态信息。

单一的Master节点策略大大简化了GFS的设计，通过Master拥有的全局Chunk信息，可以完成Chunk位置的定位以及Chunk的复制决策。

但是，单一的Master节点也引来的另外一个问题：必须减少对Master节点的读写，避免Master节点成为系统的瓶颈。GFS的解决方式是：客户端并不通过Master节点直接读写文件，而是通过向Master节点获取需要访问的Chunk的具体位置后，客户端直接与对应的Chunk服务器进行通信，与此同时，**客户端会对这些元数据信心进行一段之间的缓存**

通过Figure1. GFS Architecture描述一次简单的读取流程：

1. 客户端把**文件名**和**指定的字节偏移**（类似POSIX的read调用），根据固定的Chunk大小，传换成文件的Chunk索引。
2. 把Chunk索引和文件名发送的Master节点。
3. Master节点将相应的Chunk标识和副本的位置信息返回给客户端。
4. 客户端在本地构建一个key=Chunk索引+文件名，value=Chunk位置的缓存结构
5. 客户端发送请求到其中的一个副本处，一般选择最近的。请求信息包含了Chunk的标识和字节范围

**一切为了效率**

当客户端向Master请求Chunk的具体位置时，Master不仅仅只是将对应Chunk的具体位置发送给客户端，同时也会将该Chunk后面的Chunk信息也发送给客户端。这些额外的信息在没有任何代价的情况下，避免了客户端和Master节点未来可能发生的几次通讯。**类似read的预读**。

## Chunk尺寸

GFS选择了64MB作为Chunk的默认大小，着远大于一般文件系统的Block大小，由背景知识可知，这是合理的。

需要注意的是：每个Chunk的副本都已文件的形式保存在Chunk服务器上，**采用惰性空间分配策略**，只有需要的时候才对其进行扩大。这很好的避免的内部碎片导致的空间浪费。

现在来看64MBChunk带来的优点：

1. 避免了Master节点成为性能瓶颈，Master需要管理所有的Chunk，采用大尺寸的Chunk可以减少元数据信息规模。
2. 减少了客户端和Master节点通讯的需求，只需要一次和Master节点通信就可以获取Chunk的位置信息，之后可以对同一个Chunk进行多次的读写操作。
3. 减少了客户端与chunk server之间的网络开销，在一个chunk上往往会进行多次操作，采用tcp连接可以减少网络开销

缺点：

1. 即使结合惰性空间分配策略，采用较大的Chunk尺寸也有其缺陷，如果一个文件对应的Chunk很少，举例来说，如果文件只对应了一个Chunk，那么存储这个Chunk的Chunk服务器就会变成访问热点。
2. 一个可执行文件被写入GFS,然后数百个client同时使用它,那么存储这个可执行文件对应chunk的chunk server将会面临大量的同时方式.解决方法:**使用高副本因子**

## 元数据

Master服务器存储三种主要类型的元数据：

- 文件和Chunk的命名空间 (将文件组织成为目录)
- 文件和Chunk的对应关系
- Chunk的具体位置

前两种类型的元数据同时也会以**记录变更日志**的方式记录在操作系统的系统日志文件中，日志文件保存在本地磁盘上，同时日志会被复制到备份Master节点上。采用日志变更的方式，可以**简单可靠的更新Master服务器的状态，并且不用担心Master服务器崩溃导致数据不一致的风险**。与之相反，Master服务器**不会持久保存Chunk的位置信息**，当Master服务器启动时，或者有新的Chunk服务器加入时，向各个Chunk服务器轮询问它们所存储的Chunk的信息。

抛出一个问题：**Master如何将文件组织成为目录？**

因为元数据保存在内存中，所以Master服务器对这些元数据的操作会非常快。Master通过在后台周期性的扫描已保存的全部状态信息，这种周期性的扫描用于实现：

- Chunk的垃圾收集
- 当Chunk服务器失效的时候重新复制数据
- 通过Chunk的迁移实现跨Chunk服务器的负载均衡
- 统计磁盘使用情况

将元数据全部保存在内存中的缺陷：

Chunk的数量以及整个系统的承载能力都受限于Master服务器所拥有的内存大小。

## chunk位置信息

Master服务器并不保存持久化哪个Chunk服务器存有指定的Chunk副本信息，它只是在启动的时候轮询Chunk服务器以获取这些信息。那么如何保证这些信息的有效性呢？

- 首先Master服务器空间了所有Chunk位置的分配
- 周期性的心跳信息可以帮助Master服务器获取最新的信息

## 操作日志

操作日志包含了关键的**元数据变更**历史,这种日志的作用:

- 持久化元数据记录
- 定义了并发操作的逻辑时间序

当master服务器奔溃使,通过这些日志可用重新构建前两种类型的元数据

为了减少重建开销,采用checkpoint方式对日志进行压缩

创建checkpoint是一个耗时操作,为了不停服务,GFS采用新创建一个日志文件,并**创建一个线程完成checkpoint**,后续的操作将会在记录到**新的日志文件中**.

## 一致性模型

文件命名空间的修改（例如文件创建）是原子性的。它们仅仅由Master节点控制：

- **命名空间锁**提供了原子性和正确性保障
- Master节点的操作日志定义了这些操作在全局的顺序

![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/gfs/consiste_model.png)

- consisted：一致是**针对服务器**而言的，如果文件在各个服务器上呈现处一致的状态，那么称其为consisted
- defined：defined是**针对客户端**而言的，如果每次客户端的访问都能access到最新的数据

**举例：**

- 对于串行写，A ("fileA", offset=[30, 50])，此时A去读，一定可以读到fileA对应的[30, 50]的最新数据

- 对于并行写，A ("fileA", offset=[30, 50])，此时B同时写 B("fileA", [40, 60])，那么如果A去读的话，可能读到自己写的[30, 50]，也可能得到自己写的[30, 40]以及B的[40, 50]，但是各个replica上对应的data是一致的

- 对于记录追加，由于GFS保证每个副本至少成功一次，所以可能出现某个replica追加失败但是其他replica追加成功，于是为了保证at least once特性，只能重试这个操作，所以服务器上可能出现不一样

  - 串行：第一次追加请求执行了一半失败了，这个chunk的所有副本现在是这样：

    ![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/gfs/append_01.png)

    重复尝试后：

    ![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/gfs/append_02.png)

    对于上图，中间的数据是inconsistent，但是尾部的数据的define的

  - 并行：clientA与clientB并行的追加a和b，primary接受到两个request并将其序列化，如下图：

    ![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/gfs/append_03.png)

    追加b失败，追加a成功，于是clientB重试，如下图：

    ![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/gfs/append_04.png)

# 系统交互

设计GFS时，一个最重要的原则是**最小化所有操作和Master节点的交互**

## 租约(lease)和变更顺序

变更操作会改变Chunk内容或者元数据，比如写入操作或者记录追加操作。变更操作会在Chunk上所有副本上执行，那如何保证Chunk副本上的一致性？试想一下，client现在有Chunk的位置了，如果不对这些Chunk服务器的读写进行相关控制那如何保证一致性呢。

GFS采用租约的方式维持一致性。Master节点为Chunk的一个副本建立一个lease，这个副本叫做主Chunk。主Chunk对Chunk的所有变更操作进行序列化，所有的副本都遵循这个序列进行修改操作，这样就保证了一致性。

![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/gfs/WriteFlow.png)

从图片2中可以习得操作流程：

1. 客户端向Master节点发出询问：哪一个Chunk服务器持有当前的租约，以及其他副本的位置，如果没有一个Chunk持有租约，Master节点就选择其中一个副本建立一个租约。
2. Master节点将主Chunk的标识符以及其他副本的位置返回给客户机，客户机缓存这些数据以便后续的操作。只有在主Chunk不可用，或者主Chunk回复信息表明它已不再持有租约的时候，客户机才向Master节点再次询问。
3. 客户机把数据推送到所有副本上。**客户机可以以任意的顺序推送数据**。Chunk服务器接收到数据并保存在内存的LRU缓存中，一直到数据被使用或者过期交换出去。**由于数据流的网络传输负载比较高，通过分离控制流和数据流，可以基于网络拓扑情况对数据流进行规划，提高系统性能**。
4. 当所有的副本都确认收到了数据，客户机发送写请求到主Chunk服务器。这个请求标识了早前推送到所有副本的数据。**主Chunk为接收到的所有操作分配连续的序列号**，这些操作可能来自不同的客户机，序列号保证操作顺序执行，**值得注意的是：这个顺序不一定客户机发起操作的顺序，而是主Chunk规定的顺序，例如主Chunk以FIFO的方式接收Client的操作并编号。**
5. 主Chunk将写请求发送到所有的二级副本。每个二级副本依照主Chunk分配的序列号以相同的顺序执行这些操作。
6. 所有的二级副本回复主Chunk，它们已经完成了操作。
7. 主Chunk服务器回复客户机。

错误分析：

- 如果在主Chunk上失败，操作就不会被分配序列号，也不会被传递。
- 如果在二级Chunk上部分成功，那么客户端的请求被确认为失败，**客户端会重复执行失败的操作来处理这样的错误**。所以被修改的文件region可能处于不一致的状态。
- 如果应用程序一次写入的数据量很大，或者数据跨越了多个Chunk，GFS会将它们分成多个写操作。这些操作顺序遵循上述控制流程，但是其他客户机可能对相同的Chunk进行变更操作，这可能会出现一致的但是未定义的状态。

## 数据流

GFS采用分离数据流与控制流的方式提高网络效率。**在控制流从客户机到主Chunk、然后再到所有二级副本的同时，数据以管道的方式，顺序的沿着一个精心选择的Chunk服务器链推送。**目标是：**充分利用每台机器的带宽，避免网络瓶颈和高延时的连接，最小化推送所有数据的延时**

**如何充分利用带宽以及提高网络效率?**

- **链式推送**，每台机器的所有出口带宽都用于以最快的速度传输数据，而不是在多个接收者之间分配带宽
- **避免网络瓶颈**：每台机器都尽可能的在网络拓扑中选择一台还没有接收到数据的、离自己最近的机器作为目标推送数据
- **最小化延迟**：利用基于TCP连接的、管道式数据推送方式最小化延迟，当Chunk服务器接收到数据后，马上开始向前推送

## 原子追加操作

GFS保证记录追加是原子进行的

- **传统的写方式**，客户程序指定数据写入的偏移量。由于并发写入会导致一致但是未定义的行为的，就在服务器上所有的数据都是一致的，但是客户读取的数据不一定是想要的数据
- **记录追加方式**，为了保证原子性，所以当某个Chunk服务器出现失败的时候，会重试直到全部成功，这会导致服务器上出现不一致的情况，但是保证数据是不会被覆盖的，然后客户端可以通过一些标识得到自己想要的数据

**值得注意的是**，当客户发送请求到主Chunk服务器的时候，主Chunk服务器会检查这次操作**是否会使Chunk超过最大尺寸（64MB）**，如果超过了64MB，那么主Chunk服务器会对该Chunk进行一次padding操作，之后通知所有二级副本做同样的操作，然后回复客户端，通知其：你只能对下一个Chunk进行同样的操作了，我这已经满了。

## snapshot

快照操作可以瞬间完成对一个文件或者目录树做一个拷贝，并且几乎不会对正在进行的其他操作造成任何干扰。

GFS使用cow技术实现快照操作，当Master节点收到一个快照请求，它首先取消需要做快照的文件包含的所有Chunk的lease。这个措施保证了**后续对这些Chunk的写操作都必须与Master进行交互**，交互这一次操作十分重要，因为Master可以误为这些Chunk创建一个新的copy。具体流程如下：

1. 取消租约之后，Master节点将这个操作以日志的方式记录到硬盘上(**保证可靠性**)，然后Master节点通过复制源文件或者目录的元数据的方式，把这条日志记录的产生的副作用返回到内存保存的状态中(因为Master节点会将元数据保存在内存中，提高效率)。这里完成copy操作，**即快照文件和源文件指向相同的Chunk地址**
2. 在快照操作之后，当客户机第一次想写入数据到该Chunk，它首先会发送一个请求到Master节点查询当前的租约持有者。Master节点注意到该Chunk的引用计数超过1，此时Master节点不会马上答复客户机的请求，而是创建一个新的Chunk'元数据，并且通知拥有该Chunk的服务器为该Chunk创建一个Chunk‘的副本。通过在原Chunk服务器本地创建新的Chunk副本，减少网络通信带来的消极影响

# Master节点

- Master执行所有的命名空间操作
- 管理系统所有Chunk
  - 决定Chunk存储位置
  - 负责创建新的Chunk及其副本
  - Chunk服务器之间的负载均衡
  - 垃圾回收

## 命名空间管理和锁

Master节点的很多操作会花费很长的时间：比如在进行snapshot操作时，必须取消涉及到这次snapshot操作所有相关的Chunk的Chunk服务器的租约，很绕口啊。GFS的目标是：**在进行这一类操作时，不希望阻塞Master节点的其他操作**，采用的方式：**使用命名空间的region上的锁来保证执行的正确顺序**

不同于许多传统文件系统，GFS不支持**ls**操作，也不支持**link**操作。在逻辑上，**GFS的命名空间就是一个全路径和元数据映射关系的查找表**。利用**前缀压缩**，这个表可以高效的存储在内存中。

每个Master节点在操作之前都需要获取一系列的锁。通过情况下，如果一个操作涉及/d1/d2/.../dn/leaf，那么该操作首先需要获得目录/d1，/d1/d2，...，/d1/d2/.../dn的读锁，以及/d1/d2/.../dn/leaf的读写锁。leaf可以是目录也可以是文件。

举例，对/home/user进行snapshot操作(即将/home/user copy到/save/user)，**锁**机制如何防止创建文件/home/user/foo。快照操作获取/home和/save的**读取锁**，以及/home/user和/save/user的写入锁。然而，创建文件/home/user/foo需要获取/home、/home/user的读锁以及/home/user/foo的写锁，由于锁产生冲突，所有snapshot和creation操作会被串行执行。

采用这种锁方案的优点：

- 支持对同一目录的并行操作
- 文件名的写入锁序列化文件创建操作，确保不会多次创建同名的文件

## 副本的位置

Chunk副本位置选择的策略有两大目标:

- 最大化数据可靠性和可用性
- 最大化网络带宽利用率

**GFS采用多机架放置副本**

## 创建，重新复制，重新复杂均衡

Chunk的副本有三个用途：

- Chunk创建
- 重新复制Chunk
- 重新负载均衡

当Master节点创建一个Chunk时，它会选择在哪里放置初始的空副本，由以下几个因素决定：

1. 在低于平均硬盘使用率的Chunk服务器上存储新的副本。这样的做法最终能够平衡Chunk服务器之间的硬盘使用率
2. 限制在Chunk服务器上”最近“的Chunk创建操作数，因为创建Chunk意味着数据的写入（这里指同时在Chunk服务器上创建不同Chunk的副本数）
3. 将Chunk的副本分布在多个机架间

当Master节点**重复制**一个Chunk时，也是有上述的几个因素决定，**为了防止克隆产生的网络流量大大超过客户机的流量，Master节点对整个集群和每个Chunk服务器上的同时进行的克隆操作的数据都进行了限制**

Master服务器周期性的对副本进行**重新复杂均衡**，Master节点检查当前副本的分布情况，然后**移动副本以便更好的利用硬盘空间、更有效的进行负载均衡**。移动策略：**Master节点会选择重负载那些剩余空间低于平均值的Chunk服务器上的副本，从而平衡系统整体的硬盘使用率**

## 垃圾回收

GFS在文件删除后不会立刻回收可用的物理可用空间。GFS空间回收采用惰性的策略，只在Master进行定期垃圾回收操作时才会真正意义上的进行回收。

## 机制

当一个文件被删除时，Master节点首先记录**delete**日志，立刻把删除操作以日志的方式持久化。然后将该文件rename为一个包含时间戳的隐藏名字。当Master节点对文件系统命名空间进行扫描时，默认会删除所有三天前的隐藏文件。

在对命名空间进行扫描时，如果Master节点发现孤儿Chunk（不被任何文件包含的Chunk），直接删除其元数据。并在和Chunk服务器的心跳信息中包含那些元数据已经不在的Chunk，这样Chunk服务器可以删除这些Chunk。

GFS中的垃圾回收总结：

- GFS通过存储在Master服务器上的文件到Chunk的映射表，可以轻易的得到文件的所有Chunk
- Chunk以文件形式存储在Chunk服务器上，所以也可以轻易的得到所有Chunk的副本
- 垃圾回收提供了一致的、可靠的清除无用副本的方法
- 垃圾回收把空间操作的回收操作合并到Master节点的后台操作中，均摊开销

## 过期失效的副本检测

当Chunk服务器失效时，存储在其上的Chunk副本可能因错失了一些修改操作而过期失效。Master节点在和Chunk续约时会增加版本号，并且Master节点和Chunk的所有副本都会把新的version number持久化，**通过版本号来区分当前副本和过期副本**。

----

1. 为什么在GFS中的客户端以及Chunk服务器都不需要缓存文件数据？
   - 客户端不需要缓存数据的原因：大部分程序要么以**流的方式**读取一个大文件，要么工作集太大根本无法被缓存。
   - Chunk服务器不需要缓存数据的原因：Chunk被以文件的形式保存在本地文件系统的中，利用文件系统的特性即可。
   - **由于不需要缓存数据，所以不需要考虑一致性问题**。不过，客户端会缓存元数据。
2. 为什么Chunk服务器默认采用三副本？
   1. 从可靠性角度出发
3. 为什么GFS设计采用控制流分离的方式？
   1. 因为数据流的网络传输负载比较高，通过控制流，可以对数据流进行规划，择优传输数据
4. 为什么GFS采用64MB这种大size对数据进行分片？
   1. google内部的工作负载决定的。
5. 为什么需要设计租约？
   1. 设计租约的目的是为了最小化Master节点的管理负担
6. GFS是如何通过Checksum完成忽略record append操作带来的padding以及重复记录？
   1. **解决padding**，可以在有效记录前面增加一个magic number或者checksum标识这条record是非padding
   2. **解决重复**，在record中包含一个unique id，如果发现这天record的id和前一条的一样，那么就可以确认这条record是重复的
7. GFS为什么采用多机架放置副本？
   1. GFS为了最大化数据可靠性和可用应，最大化网络带宽利用率，选择将副本放置于不同机架间。如果仅仅只是将副本放置于多台机器上，这只能预防硬盘损坏或者机器失效带来的影响，以及最大化每台机器的网络带宽利用率。**如果整个机架被破坏，这会导致服务不可用**，当然有得必有失，虽然多机架副本策略提高了可用性以及最大化了网络带宽利用率，但是**写操作必须和多个机架上的设备进行通信**
8. GFS在什么时候会创建Chunk副本？
   1. 当需要新的Chunk进行写入操作时
   2. 当Chunk的有效副本数少于用户指定的复制因数时
      1. 一个Chunk服务器不可用了
      2. Chunk服务器向Master节点报告它所存储的一个副本损坏了
      3. Chunk服务器的一个磁盘不可用了
      4. Chunk副本的复制因数提高了
9. 为什么不将chunk location这一类元数据持久化保存在master上?
   1. 避免master和chunk server之间的同步,这种同步发生在:chunk server加入或离开集群,chunk server改名,失败,重启等.
   2. 在master启动时,向chunk server轮询chunk location+定期轮询相对来说简单的多,因为不需要同步操作.
   3. 进一步来说,在master上持久化保存chunk location没有意义,因为chunk server才是决定chunk是否存在的唯一因素,考虑chunk server被重命名了或者chunk server上chunk由于某种错误消失了,那此时master上的信息将是无意义的.
10. 为何并发append失败，仍然能够保证defined？
    1. 因为GFS保证Append的原子性(出现失败会重试)
11. GFS应用程序如何在宽松一致性模型下良好的工作？
    1. 通过append而不是overwrite
    2. checkpoint
    3. write self-validating
    4. self-identify records
12. 客户端会不会读取到过时或失效的数据？
    1. 写入操作成功：
       1. 客户端缓存了chunk location元数据，如果chunkserver出现数据过时(如chunkserver重启)，那么客户端将读到过期的数据
       2. 从shadow master可能会读到过期的元数据，所以可能会读到过期的元数据
    2. 写入失败：
       1. 写入失败肯定会导致inconsistent
13. 客户端读取到错误数据怎么办？
    1. 如果客户端读取某个chunk的region，chunkserver发现该chunk的checksum出现错误，那么将返回错误给客户端并向master节点汇报
    2. 客户端此时访问其他副本，服务器将一个正确的chunk replica复制到该chunkserver上并通知chunkserver将原先的chunk删除

---

**参考文献**

[1]. The Google File System

[2]. Google File System 中文版
