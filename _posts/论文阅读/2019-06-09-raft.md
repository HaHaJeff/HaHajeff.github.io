---
?layout: post
title: raft算法
category: 论文阅读
tags: [论文阅读]
keywords: raft, 分布式共识性算法
---

# 背景

共识性算法是指允许一组机器在有机器无法提供服务时，仍能保证这组机器的一致性

# raft算法流程

- follower
- candidater
- leader

**raft采用强leader保证算法的简洁性以及正确性**

![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/raft/all.png)

## leader选举

**raft使用心跳出发leader选举。当服务器启动时，初始化为follower。leader向followers周期性发送heartBeat。如果follower在选举超时时间内没有收到leader的heartBeat，就会等待一段随机的时间后发起一次leader选举**

随机等待的目的是**减少选举冲突**。

- 只有包含最新**commited log**的candidater才能竞选成功，raft如何保证？
  - RequestVote会携带prevLogIndex以及prevLogTerm，其他节点在收到消息时：
    - 如果发现自己的日志比请求中携带的更新，则拒绝投票。比较原则：如果本地的最后一条的log entry的term更大，则term大的更新。如果term一样大，则log index更大的更新
    - 否则，同意该candidater的选举

**leader选举在raft中表现为RequestVote RPC，流程如下：**

![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/raft/requestVoteRpc.png)

## 日志复制

leader把客户端请求作为日志条目，然后并行的向其他服务器发起AppendEntries RPC复制条目。当这条日志被复制到大多数服务器上时，leader将这条日志应用到它的状态机中，并向client返回结果。

raft日志同步保证以下两点：

- 如果不同日志中的两个条目有着相同的索引号和任期号，则它们所存储的命令相同
- 如果不同日志中的两个条目有着相同的索引号和任期号，则它们之前的所有条目都是一样的

第一特性源于leader在一个term内，在一个给定的log index最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变

第二条特性源于AppendEntries的一个简单的一致性检查。当发送一个AppendEntries RPC时，Leader会把新日志条目紧接着之前的条目的log index和term都包含在里面。如果Follower没有在它的日志中找到log index和term都相同的日志，它就会拒绝新的日志条目。

- leader通过强制followers复制它的日志来处理日志的不一致，followers上的不一致的日志会被leader的日志覆盖。
- leader为了使followers的日志同自己的一致，leader需要找到followers同它的日志一致的地方，然后覆盖followers在该位置之后的条目。
- leader会从后往前尝试，每次AppendEntries失败后尝试前一个日志条目，直到成功找到每个follower的日志一致位点，然后向后覆盖

**日志复制在raft中表现为AppendEntries RPC，流程如下：**

![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/raft/appendVote.png)


## 状态
![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/raft/state.png)

- 需要保存的状态：currentTerm，votedFor，log[]
- 需要变化的状态：commitIndex，lastApplied
- 在leader上变化的状态：nextIndex[]，matchIndex[]

## 安全性

- 拥有最新的已提交的log entry的follower才能成为leader
- leader只能推进commit index来提交当前term，旧term日志的提交要等到当前term的日志间接提交



# Client interaction

## find leader

- 当client启动时，随机的连接一个server。如果该server不是leader，那么client的request将会被拒绝，同时得到server的response，该response中包含了最近一次的leader位置
- client向leader发起连接，如果leader已经crash了，那么client的request将会超时，此时，client回到第一步

## linearizable

**定义：**在线性一致性系统中，任何操作都能在调用和返回之间原子的执行完成

- 瞬间完成（原子性）
- 发生在invoke与response两个事件之中
- 反映出“最新值”

![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/raft/linearizable.png)

上图满足线性一致

- 对于x，其初始值为1，客户端ABCD并发的进行请求

- 对于每个请求，线段表示处理request花费的时间，线性一致性没有规定request具体在线段上的哪个点执行，只需要在**invode与response之间**的某个时间点**原子**的执行完成

- 最新的值，从client B的角度看：

  ![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/raft/example_B.png)

  

  x的值分为三个阶段，在invoke w(2)和response之间对x的读取可能是1也可能是2，但是在response之后对x的读取一定是2

## raft的线性一致

- 对于write操作，leader会生成对应的op log，并将其序列化，以此顺序commit，并apply后返回client，当write被commit以后，raft保证其前面所有的write已经被commit。所以，所有的write操作都是严格有序的。综上，raft满足线性一致写

- 对于read操作，采用read log的方式一定是满足线性一致读的，但是这样的话有点小题大作（开销很大），因为read操作其实并不改变状态机

  - 使用**read index**减少开销，由于read log需要涉及到网络以及磁盘开销，采用read index可以减少磁盘开销，从而提升效率。具体实现方式：

    1. leader记录当前的commit index，称为read index
    2. 向follower发起以此心跳，证明自己还是leader  **十分必要**
    3. 等待状态机**至少**应用到read index记录的log
    4. 执行read，将结果返回给client

    ![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/raft/linearizable_leader_valid.png)

    第二点的必要性，考虑ABCDE5个节点

    1. 刚开始的时候A作为leader对外提供服务
    2. 发生网络隔离，集群被分割成两部分，A和B以及CDE。虽然A会持续向其他几个节点发送heartbeat，但是由于网络隔离，CDE将无法收到A的heartbeat。默认的，A不处理向follower节点发送heartbeat失败(此处为网络超时)的情况(协议没有明确说明heartbeat是一个必须收到follower ack的双向过程)
    3. CDE组成的分区在经过一定时间没有收到leader的heartbeat后，触发election timeout，此时C成为leader。此时，原来的5节点集群因网络分区分割成两个集群：小集群：A和B，A为leader；大集群：C、D和E，C为leader
    4. 第三点中的**至少**是关键要求，因为：read log可以保证线性一致性读，该请求发生的点为commit index，也就是说这个点能使read log满足线性一致，那显然发生在这个点之后的read index也能满足线性一致读

  - 使用**lease read**，lease read与read index类似，但更进一步，不仅省去了磁盘log，还省去了网络交互。它可以大幅度提升读的吞吐也能显著降低延时。

    - 基本思路：leader取一个比election time out更小的租期，在租期内不会发生选举，确保leader不会变，所以可以跳过read index的第二步，也就降低了延时。lease read的正确性和时间挂钩，因此时间的实现至关重要，如果漂移严重，这套机制就会有问题

  - **wait free**，lease read省去了read index的第二步，实际还能再进一步，省去第三步。这样的lease read在收到请求后立刻进行读请求，不取commit index也不等状态机。由于raft的强leader特性，在租期内的client收到的response由leader的状态机产生，所以只要状态机保证线性一致，那么在lease内，不管任何时候发生读都能满足线性一致。

---

1. 为什么wait-free可以满足线性一致性读？

   1. raft保证leader一定拥有最新的commit数据，但是leader的状态机数据不一定是最新的(例如，leader刚刚由follower转换为leader，其committed index之前的数据不一定应用到状态机中)
   2. read log以及read index都是通过保证apply index >= committed index，也就是说，保证在read log发生之前的committed index全部被应用到状态机即可，也就是说保证状态机即可保证read线性一致
   3. leader拥有最新的committed index，那么只要leader中该term的第一条log被应用到状态机中就可以保证状态机最新了(后续的写操作一定是被apply之后才返回的，所以只要leader的第一条记录被应用到状态机中就能保证leader的状态机最新)
2. raft如何保证leader拥有最新的日志？
   1. 在RequestVote阶段保证只有最新的Candidate能够选举成功。
      1. RequestVote会携带prevLogIndex以及prevLogTerm，其他节点在收到消息时：
      2. 如果发现自己的日志比请求中携带的更新，则拒绝投票。比较原则：如果本地的最后一条的log entry的term更大，则term大的更新。如果term一样大，则log index更大的更新
      3. 否则，同意该candidater的选举

---

1. Raft为了简洁性牺牲了什么？
   1. raft为了简洁性放弃了部分性能
      1. 每一个操作必须在磁盘上进行持久化，通过batch优化
      2. 拒绝乱序提交，通过pipeline优化
      3. server无法充分利用多核优势，因为必须顺序执行操作
2. raft如何优化？
   1. 首先确定raft的一次运行流程：
      - leader收到client发送的request
      - leader将request append到自己的本地log
      - leader将log entry发送给其他的follower
      - leader等待follower的结果，如果大多数follower提交了这个log entry，则leader对其进行apply
      - leader将结果返回给client
   2. batch优化
      1. 第四步apply log时，可以先缓存起来，然后一起进行apply，减少落盘影响
      2. 但是需要注意的是，如果已经向client返回了，但是leader突然宕掉，此时可能会导致数据丢失，所以需要进行redo，在leader重启时先恢复
   3. pipeline优化
      1. batch优化依旧会导致leader需要等到follower返回时才能进行下一步工作，采用pipeline可以避免
      2. leader会维护一个nextindex[]数组记录每一个follower的next log entry index，在等待follower返回后对数组进行更新，采用pipeline即是在leader发送完log entry后就更新nextindex数组，此时leader可以开始发送下一批log
      3. pipeline存在的问题是：上一次的index返回失败，而这一次的index成功了可怎么办？**leader会重新更新nextindex数组，同时重新发送log**
   4. Append log parallelly？

      1. 对leader append操作和leader将log发送给其他的follower可以并行操作，原因是：当一个log被大多数节点append了，这条log entry就可以被认为是committed，committed的log entry一定可以被apply，所以即使leader宕机了，依旧能够保持正确性
      2. leader在收到大多数follower的append成功之后就可以认定这个log entry被committed了，所以可以对apply操作进行异步

   5. raft和multipaxos的区别？
      1. raft和multipaxos在选举完leader之后除以下差别外，其余基本一致：
         1. raft仅允许日志最多的节点当选为leader，而multi-paxos则相反，任意节点都可以当选leader
         2. raft不允许日志的空洞，这也是为了比较方便和拉平两个节点的日志方便，而multi-paxos则允许日志出现空洞，也就是说raft不允许乱序commit，而paxos允许乱序commit，但是两者都不允许乱序apply
   6. match_index与next_index？
---

# 参考

[1]. [In Search of an Understandable Consensus Algorithm](https://raft.github.io/raft.pdf)

[2]. [raft youtube](https://www.youtube.com/watch?v=JEpsBg0AO6o&t=182s)

[3]. [etcd线性一致性读](https://zhengyinyong.com/etcd-linearizable-read-implementation.html)

[4]. [线性一致性和raft](https://pingcap.com/blog-cn/linearizability-and-raft/)
