---
?layout: post
title: BigTable论文阅读
category: 论文阅读
tags: [分布式系统]
keywords: 分布式系统, google
---

---

# BigTable

Bigtable是一个基于GFS的**分布式结构化数据存储系统**，设计目的式处理**海量数据**，论文中提到数据量在PB级别

![BigTable架构图](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/bigtable/bigtable_architecture.jpg)

## 数据模型

Bigtable是一个**稀疏的、分布式的、持久化的多维度排序Map**，Map的索引是**行关键字、列关键字以及时间戳**；Map中的每一个value都是一个二进制字符串

> (row: string, column: string, time: int64)->string

### 行

​    行关键字可以是任意的字符串，最大支持64KB。**对同一个行关键字的读或者写操作都是原子的**，也就是说Bigtable的事务单位是：行，这也是bigtable的诟病之一吧，不支持**分布式事务**

​    Bigtable通过**row**关键字的字典顺序来排列数据，Bigtable管理数据的单位是tablet(类似GFS中的Chunk)，负责**数据分布以及负载均衡**。例如，通过反转URL中主机名的方式，可以把同一个域名下的网页聚集起来组织成连续的行，这样可以加速网页的加载。

### 列族

​    列组成的集合叫做**列族**，列族是访问控制的基本单位。存放在同一个列族下的所有数据通常属于同一个类型。列族在使用之前必须先创建，然后才能在列族中创建列存放数据。

​    列关键字的语法如下

> 列族名 : 限定词

列族名必须是可以打印的字符串，而限定词则可以是任意形式的字符串

## 时间戳

- 在Bigtable中，表中的每一个数据项都可以包含同一份数据的不同版本
- 不同版本的数据通过时间戳来索引，且这些数据按照时间戳倒排列

## 基本组件

- 使用**GFS**存储日志文件和数据文件
- 存储格式采用**SSTable**，SSTable是一个持久化的、排序的、不可更改的key-value数据结构
- 依赖分布式锁服务组件**Chubby**

### SSTable

**逻辑结构**——基于LevelDB

> - **data block**: 用来存储key value数据对；
> - **filter block**: 用来存储一些过滤器相关的数据（布隆过滤器），但是若用户不指定leveldb使用过滤器，leveldb在该block中不会存储任何内容；
> - **meta Index block**: 用来存储filter block的索引信息（索引信息指在该sstable文件中的偏移量以及数据长度）；
> - **index block**：index block中用来存储每个data block的索引信息；
> - **footer**: 用来存储meta index block及index block的索引信息；

​    ![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/bigtable/sstable_logic.jpeg)

​    如上图所示：

​    **footer**用来索引**index block**和**meta index block**，而**index block**则用来索引data block，**meta index block**用来索引**filter block**

### Chubby

Bigtable使用Chubby完成以下几个任务:

1. 确保在任何给定的时间内最多只有一个活动的Master副本
2. 存储tablet的bootstrap数据(即下文中的第一层元数据)
3. 查找tablet server，负责tablet server失效的善后工作
4. 存储Bigtable中每张表的列族信息
5. 存储访问控制列表

## 实现

Bigtable实现主要包含了三个组件

- 连接到客户程序的库
- 一个Master服务器
- 多个Tablet服务器

### Master服务器

负责工作：

- 为tablet服务器分配tablets
- 指导子表服务器进行子表的合并
- 接受来自子表服务器的子表分裂消息
- 检测新加入的或者过期失效的tablet服务器
- 对tablet服务器进行负载均衡
- 字表服务器的故障恢复
- 对保存在GFS上的文件进行垃圾收集
- 除此之外，负责诸如建立tablet以及建立列族等

客户端需要读写数据时，直接与Tablet服务器联系。因为客户端不需要从Master服务器获取片的位置信息，所以大多数客户端从来不需要访问Master服务器。**HBase就是BigTable的开源实现，在HBase中也是如此，只有在创建Table时需要访问Master服务器**

### Tablet服务器

- 每个tablet服务器负责一系列tablet，**tablet的数据存储在GFS中**
- 负责这些tablets的读写操作以及tablets的合并、分裂操作

与GFS类似，客户端读取tablet服务器的数据不需要经过Master服务器，客户程序直接和Tablet服务器进行通信

每个tablet包含了属于某个范围的行，其尺寸大约时100MB——200MB

### Tablet位置

​    使用一个三层的、类似B+树的结构存储Tablet的位置信息

​    ![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/bigtable/hierarchy.png)

1. 第一层是一个存储在Chubby中的文件，叫做根表，它存储了每个METADATA表的位置
2. 第二层是一系列METADATA表的集合，每个METADATA表又由很多条记录构成，每条记录由tablet的标识符及其最后一行编码组成，子表位置信息、SSTable以及操作日志文件编号、日志回放点等等
3. 第三层则由一些列具体的tablet组成，每个tablet存储了某个范围内的行记录

> ​    假设一个tablet大小128MB，每一个tablet对应的元数据大小为1KB，那么一个128MB的METADATA表能够存储128MB/1KB条记录，这些记录对应的tablets大小为128MB/1KB*128MB=16TB，三层架构下能够索引的tablets的大小的16TB*128MB/1KB=2048PB，满足几乎所有业务的需求

​    客户端查询时，首先从Chubby中读取根表位置，接着从根表读取所需的元数据字表的位置，最后就可以从元数据字表中找到待查询的用户字表的位置。为了减少访问开销，客户端使用缓存和预读技术，最好情况下，一次查询不需要通信，然而，如果缓存为空，则需要三次通信开销。如果缓存过期，则需要六次通信开销

### Tablet分配

​    在某一个时刻，一个Tablet只能分配给一个Tablet服务器。*Master服务器记录了当前有哪些**Tablet服务器有效**、哪些**Tablet还没有被分配***。

​    当一个Tablet还没有被分配并且刚好有一个Tablet服务器有足够的空闲空间装载该Tablet时，Master服务器会给这个Tablet服务器发送一个请求，把这个Tablet分配给这个服务器。

​    Bigtable使用Chubby跟踪记录Tablet服务器的状态。当一个Tablet服务器启动时，它在Chubby的一个指定目录下建立一个唯一名字的文件，并且获取该文件的独占锁。Master服务器实时监控这个目录，因此Master服务器能够知道有新的Tablet服务器加入了。如果Tablet服务器出现故障，Master服务器需要等到Tablet服务器的互斥锁失效，才能把它上面的tablet迁移到其他Tablet服务器。

​    Master服务器会定期询问Tablet服务器是否持有锁，如果Tablet服务器的锁丢失或者没有回应，则此时可能出现两种情况：

- Chubby出现了问题
- Tablet服务器出现了问题

​    对此，Master服务器会尝试获取这个锁，如果获取成功，则Chubby是有效的，否则，Chubby失效。如果获取成功，Master服务器将终止这个Tablet服务器并将其上的子表全部迁移到其他Tablet服务器。

​    当Master服务器启动后，Master服务器首先要了解当前Tablet的分配状态，之后才能够修改分配状态。Master服务器在启动的时候执行以下步骤：

1. Master服务器从Chubby获取独占锁，**用来组织其他的Master服务器实例**
2. Master服务器扫描Chubby的服务器文件锁目录，获取当前正在运行的服务器列表
3. Master服务器和所有的正在运行的Tablet服务器通信，获取每个Tablet服务器上正在被服务的Tablet
4. Master服务器扫描METADATA表获取所有的Tablet的集合

​    在扫描METADATA表的过程中，如果Master服务器发现了一个还没有被分配的Tablet，那么就将这个Tablet加入到未分配的Tablet集合中

​    可能会遇到一种复杂的情况：在METADATA表的Tablet还没有被分配之前是不能够扫描它的。因此在开始扫描之前(步骤4)，如果在第三步的扫描过程中发现Root Tablet还没有被分配，那么Master服务器就会将这个Tablet加入到未分配的Tablet集合。这个附加操作**保证了Root Table会被分配**。由于Root Tablet包括了所有METADATA的Tablet的名字，因此在Master服务器扫描完Root Tablet以后，就得到了所有的METADATA表的Tablet的名字了。

​    现在的Tablet集合只有在以下事件发生时才会改变：

- 建立了一个新表或者删除了一个旧表
- 两个Tablet被合并了
- 一个Tablet分裂了

​    Master服务器可以跟踪记录上面提到的前两个事件，因为这两种事件是由Master服务器发起的，但是最后一个事件是由Tablet服务器发起的。在分裂操作完成之后，Tablet服务器通过在METADATA表中记录新的Tablet信息来往完成分裂操作的提交，因为Bigtable保证单行记录的原子性，所以只要提交成功，那么METADATA表中必定存在新的记录，提交之后，Tablet服务器会通知Master服务器。考虑如下场景：如果在提交成功之后，Tablet服务器宕机了，那么Master服务器可能丢失汇报分裂消息。为了防止这种情况的发生，**Master服务器会在向新的Tablet服务器发出load已经被分裂的Tablet请求时**，发现这个Tablet已经被分裂了，因为对比METADATA表中的Tablet信息，Tablet服务器发现目前MEATDATA表中的信息并不完整(**因为已经分裂了**)，因此，Tablet服务器会重新向Master服务器发送通知信息

​    Bigtable的底层存储采用**GFS**，GFS本质上是一个弱一致性系统，其一致性模型只保证“同一条记录至少成功追写一次”，但是可能存在重复记录，而且可能存在一些补零记录。

> ​    Bigtable写入GFS的数据分为两种：
>
> - 操作日志。当Tablet服务器发生故障时，它上面服务的子表会被集群中的其他Tablet服务器加载继续提供服务(迁移)。加载子表可能需要回放操作日志，每条操作日志都有唯一的序号，通过它可以去除重复的操作日志
> - 每个子表包含的SSTable数据。如果写入GFS失败可以重试并产生多条重复记录，但是Bigtable只会索引最后一条写入成功的记录
>

### Tablet服务

![](https://raw.githubusercontent.com/HaHaJeff/HaHaJeff.github.io/master/img/bigtable/tablet_serving.png)

​    如上图所示，Tablet的持久化信息保存在[GFS](https://hahajeff.github.io/2019/02/18/gfs.html)中。更新操作先提交到Redo log中，在向memtable中写入数据。较早的更新存放在一系列的SSTable中。为了恢复一个Tablet，Tablet服务器首先从METADATA表中读取它的元数据。Tablet的元数据包含了组成这个Tablet的SSTable列表以及一系列的Redo point，这些Redo point指向可能含有Tablet数据的已提交额日志记录。Tablet服务器把SSTable的索引读进内存，之后通过重复Redo point之后的提交的更新来重建memtable。

​    **写操作流程：**

1. Tablet服务器首先检查这个操作是否正确、操作发起者是否有执行这个操作的权限，权限验证的方法是通过从一个Chubby文件里读取出来的具有写权限的操作列表来进行验证(这个文件几乎一定会存放在客户端的Chubby缓存中)。
2. 有效的修改操作首先记录在提交日志中，可以采用批量提交方式来提高包含大量小的修改操作的应用程序的吞吐量
3. 当写操作提交后(写入日志后)，写的内存被插入到Memtable中

​    **读操作流程：**

1. Tablet服务器同样做出类似的完成行和权限验证。
2. 有效的读操作由一系列SSTable和Memtable合并的视图完成。

​    在分裂或合并操作正在进行时，read、write操作依旧能够顺利执行。

### Compaction

- minor compaction：当memtable增大到一个阈值的时候，这个memtable会被冻结，然后创建一个新的memtable，被冻结的memtable会被转换成SSTable，然后写入GFS。
  - 收缩Tablet服务器正在使用的内存
  - 减少服务器在恢复过程中，减少必须从日志文件中读取的数据量。因为恢复过程相当于时恢复memtable，如果memtable过大，那么则需要读取很多日志文件中的内容
- merge compaction：限制minor compaction产生的SSTable文件数量。merge compaction过程中会读取一些SSTable和memtable的内容，合并成一个新的SSTable，只要merge compaction完成了，输入的这些SSTable和memtable就可以删除了。
- major compaction：merge compaction产生的SSTable可能包含已经删除的条目，major compaction过程合并所有的SSTable，并将这些SSTable中标记删除的条目进行真正的删除

## 优化

### Locality group

​    客户程序可以将多个列族组合成一个Locality group(局部性群组)，对Tablet中的每个局部性群组都会单独生成一个SSTable。将通常不会一起访问的列族分割成不同的局部性群组可以提高读操的效率。例如，在Webtable表中，网页的元数据可以在一个群组中，而网页的内容可以在另外一个群组中。当一个应用程序读取网页的元数据时，没有必要读取所有的页面内容。

### 压缩

​    通过分块压缩局部性群组中的内容，虽然分块压缩浪费了少量空间，但是，在读取SSTable的一小部分数据时就不必解压整个文件了。

### 通过缓存提高读操作的性能

为了提高读操作的性能，Tablet服务器使用二级缓存策略：

- scan cache作为第一级cache，主要缓存Tablet服务器通过SSTable接口获取的Key-Value对
- block cache作为第二级cache，缓存的时从GFS读取的SSTable的block

对于需要反复读取相同数据的应用程序而言，scan cache非常有效；对于空间局部性较强的应用程序而言，blocka cache则更有用

### bloom 过滤器

​    一个读操作必须读取构成Tablet的所有的SSTable数据，如果这些SSTable不在内存中，那么就需要多次访问硬盘。通过bloom过滤器判断一个SSTable是否包含了特定行和列的数据，这样，只付出了少量的、用于存储bloom过滤器的内存的代价，就换来了读操作显著减少的磁盘访问的次数。

### Commit日志的实现

​    如果为每个Tablet都生成一个Commit日志文件，那么就会产生大量的文件。另外，由于批量提交中的操作数目比较少，所以也体现不出来优势。为了避免这些问题，采用每个Tablet服务器一个commit日志文件。

​    然而，这也复杂了恢复操作，因为一个日志文件中包含了多个Tablet的commit日志。例如，当一个Tablet服务器宕机时，它加载的Tablet会被迁移到很多其他的Tablet服务器上，每个Tablet服务器都load很少的原来服务器的Tablet，并执行恢复操作。由于，这些Tablet的commit日志全部在一个文件中，所以同一个文件会被加载很多次，因为每次恢复都需要读取完整的日志文件。

​    为了避免多次读取日志文件，首先将日志按照关键字(table, row name, log sequence number)排序。排序之后，对同一个Tablet的修改操作的日志记录就连续放了，因此，只需要一次磁盘seek操作，之后顺序读取。为了并行排序，将日志文件分割成64MB的段，之后在不同的Tablet服务器对段进行排序。这个排序工作由Master服务器协同处理，发生时机在：Tablet服务器表明自己需要从Commit日志文件中恢复Tablet时。

### Tablet快速恢复

​    当Master服务器将一个Tablet从一个Tablet服务器转移到另外一个Tablet服务器时，源Tablet服务器会对这个Tablet做一次Minor Compaction。这次Minor Compaction减少了恢复需要的时间，因为需要读取的日志内容变少了(第一次Minor Compaction不会停止服务)。当第一次Minor Compaction完成以后，该Table服务器就停止为该Tablet提供服务了。在卸载Tablet之前，源Tablet服务器还会再做一次Minor Compaction，以消除上一次Compaction过程中累计的为被写入SSTable中的记录。第二次Minor Compaction完成以后，Tablet就可以被load到其他Tablet服务器上去了，并且不再需要恢复。

问题：为什么还需要第二次minor compaction，即使这会导致Tablet服务器停止服务？

### 利用不变性

​    BigTable中，除了SSTable caches，其他的SSTable都是不可变的。所以，当从SSTable读取数据的时候，不必为文件系统访问操作进行同步。

​    memtable是唯一一个能被读和写操作同时访问的可变数据结构，为了减少读操作时的竞争，对memtable的row采用COW(Copy On Write)机制，这样就允许读写操作并行执行。

​    SSTable的不可变性，也简化了删除操作，采用垃圾收集操作对废弃的SSTable进行回收。Master服务器采用标记删除的垃圾回收方式删除SSTable集合中废弃的SSTable。

​    SSTable的不变性同时简化了Tablet的分裂，不必为分裂Tablet建立新的SSTablet集合，而是共享原来的SSTable集合，仅仅只是在内存中将memtable一分为二，在Compaction阶段再生成新的SSTable，同时删除旧的SSTable。

---

1. 为什么需要minor compaction?
   1. 当memtable的写入量超过阈值时，进行minor compaction，将memtable将其写入sstable
   2. 收缩tablet server的内存使用率
   3. 在服务器从宕机中恢复过程中，减少该操作需要读取的commit log的数据量

2. 为什么需要merge compaction?
   1. 不断的minor compaction会造成许多的sstable，所以一个read操作可能需要读作很多的sstable
   2. 为了减少sstable的数量，采用merge compaction，merge compaction需要读取memtable和一些sstable并生成一个新的sstable，完成新的sstable的产生，这些memtable以及sstable就可以删除了

3. 为什么需要major compaction?

   1. 对所有的sstable进行一次合并，minor compaction和merge compaction生成的sstable可能包含已经删除的数据(删除操作不会真正执行，而是采用一条记录进行标记)

4. 为什么需要columns family?

   1.  列族标识一系列相似的列的集合，将这些列放在一起配合Locality group可以提高读效率

5. 为什么采用tablet管理数据?

   1. 将一定范围内的行数据存储到一个tablet中，这样当需要读取一定范围的行数据时，十分高效。

6. Master服务器如何知晓Tablet服务器加入的？

   1. 通过监控Chubby某个目录下的文件可以知晓有哪些Tablet服务器，因为每一个Tablet服务器加入都会在该目录下创建一个文件并获取该文件的独占锁

7. 为什么Tablet服务器需要获取互斥锁？
   1. 为了保证同一个Tablet只能被一台Tablet服务器服务，当Tablet服务器失效时，Master服务器会等到Tablet服务器的互斥锁失效时才将上面的Tablet进行迁移
   2. 个人见解，GFS中客户端缓存会导致客户端读到过期的数据，但是由于互斥锁的存在，即使客户端缓存了Tablets的位置信息，那么客户端也不能直接与未包含互斥锁的TabletServer进行通信

8. Master服务器如何知晓Tablet服务器上已经分配的Tablet？
   1. 通过查看Chubby中的某个特殊目录，该目录中包括的信息有：目前活跃的Tablet服务器，以及每个Tablet服务器上现已分配的子表

9. 如何理解Tablet服务器宕机后的子表迁移?
   1. 其实我觉得迁移这个词有点不当，"迁移"并没有涉及到Tablet服务器之前的数据转移，因为数据全部存储在GFS中，"迁移"只是表示：tablet服务器上的tablets现在有别的tablet服务器负责
   2. “迁移”的方式与子表持久化有关，子表持久化两样东西：
      1. 操作日志
      2. SSTable
      3. 两者都是存储在GFS中，所以如果Tablet服务器失效，那么Master服务器只需要读取操作日志以及SSTable即可恢复原Tablet服务器内存中的数据(Memtable)
        Tablet服务器并没有为每一个Tablet维护一个操作日志文件，而是把所有它服务的tablet的操作日志写
      4. 在一个文件，每条日志通过<表格编号，行主键，日志序列号>来唯一标识。为了减少恢复过程，Tablet服务器需要从GFS中读取的日志文件数据量，Master服务器会选择一些Tablet服务器对日志进行排序，排好序后，同一个子表的操作日志连续存放。选择方案是：低负载的Tablet服务器是首要目标

10. 为什么在合并或分裂操作正在执行时，读写操作依旧能够顺利执行?
    1. 分裂操作只需要将内存的索引信息分成两份即可。比如分裂前子表的分为为(起始主键，结束主键]，在内存中将索引分成(起始主键，分裂主键]，(分裂主键，结束主键]两个范围。分裂以后两个子表各自写不同的Memtable，等到执行Compaction操作时再根据分裂后的子表范围生成两个不同的SSTable，无用的数据自然成为垃圾被回收
    2. 合并操作，由Master控制合并过程，过程很复杂。
    因为原始的SSTable并没有在这两个过程中被删除，Tablet服务器当然能够提供服务？

11. 如何避免刚刚生成的SSTable被删除？
    1. 垃圾回收机制，Master服务器通过扫描GFS获取所有的SSTable文件，接着扫描根表和元数据表，如果发现SSTable没有被任何子表使用，那么删除这个SSTable。对于新加入的SSTable可能还没有加入到元数据表中，为了避免这种情况，一种简单的做法是：只删除一段时间内没有被使用的SSTable。

# 参考

[1]. [Bigtable论文](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf)

[2].  [leveldb-handbook](https://leveldb-handbook.readthedocs.io/zh/latest/sstable.html#id2)

